# ğŸ” AGENT CONCERNS RESOLUTION & SPECIFICATION CORRECTIONS

**Status:** Critical Issues Identified & Answers Provided  
**Date:** December 5, 2025  
**Impact:** Affects Phase 1-4 Implementation

---

## âœ… RESOLVED: All 12 Concerns Addressed

---

## 1. ğŸ”´ CRITICAL: Missing Specification Details with Line References

**Issue:** Orchestration prompt references specific line numbers that don't align with actual document structure.

**Root Cause:** Line numbers in orchestration prompt (e.g., "Section 2.2.A (lines 185-220)") are ESTIMATES. The actual technical design document is 3,405 lines with integrated content.

**SOLUTION:** âœ… **Use content-based extraction, NOT line numbers**

### For Agent Implementation:

```
When extracting code from [END_STATE_TECHNICAL_DESIGN]:
1. Search for exact section headers (e.g., "### 3.2 Instruction Bundle XML Examples")
2. Extract content between headers, NOT using line numbers
3. Normalize escaped characters:
   - Replace \_ with _
   - Replace \\n with \n
   - Replace \\[ with [

Example:
WRONG: Extract lines 355-385
RIGHT: Find "### 3.2 Instruction Bundle XML Examples" header, extract until next "###" marker
```

**Updated Orchestration Prompt Section 2.2:**

Replace:
```
Section 2.2.A (lines 185-220) - Tool: `read_neo4j_cypher`
Section 2.2.B (lines 240-320) - Tools: `recall_memory` and `save_memory`
```

With:
```
Section 2.2.A - Tool: `read_neo4j_cypher`
  Location: Under heading "#### **A. Tool: `read_neo4j_cypher`..."
  Extract: Code block from "def read_neo4j_cypher(cypher_query: str)"

Section 2.2.B - Tools: `recall_memory` and `save_memory`
  Location: Under heading "#### **B. Tools: `recall_memory` and `save_memory`..."
  Extract: Code blocks for both functions + Cypher Query section
```

**Status:** âœ… READY - Agent must use content headers, not line numbers

---

## 2. ğŸ”´ CRITICAL: PostgreSQL Schema Inconsistency (Foreign Key Issue)

**Issue:** The `instruction_metadata` table has a FOREIGN KEY to `instruction_bundles.tag`, but `tag` is NOT defined as UNIQUE in the spec, only `UNIQUE (tag, version)`.

**Root Cause:** PostgreSQL ForeignKey constraints require the referenced column to be UNIQUE or PRIMARY KEY. The composite unique constraint `UNIQUE (tag, version)` does NOT satisfy this.

**CORRECT SCHEMA:** âœ… **Add UNIQUE constraint on tag column**

### Current (BROKEN):
```sql
CREATE TABLE instruction_bundles (
    id SERIAL PRIMARY KEY,
    tag TEXT NOT NULL,  -- âŒ NO UNIQUE constraint on tag alone
    version TEXT NOT NULL,
    UNIQUE (tag, version)  -- âŒ Composite, won't work for FK
);

CREATE TABLE instruction_metadata (
    tag TEXT REFERENCES instruction_bundles(tag),  -- âŒ FK WILL FAIL
    PRIMARY KEY (tag)
);
```

### Corrected Schema:
```sql
CREATE TABLE instruction_bundles (
    id SERIAL PRIMARY KEY,
    tag TEXT NOT NULL,
    path_name TEXT,
    content TEXT NOT NULL,
    category TEXT,
    avg_tokens INTEGER,
    version TEXT NOT NULL,
    status TEXT NOT NULL,
    experiment_group TEXT,
    depends_on TEXT[],
    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,

    -- âœ… UNIQUE on tag (allows FK references)
    UNIQUE (tag),
    
    -- âœ… UNIQUE on (tag, version) for versioning
    UNIQUE (tag, version)
);

CREATE TABLE instruction_metadata (
    tag TEXT PRIMARY KEY REFERENCES instruction_bundles(tag),  -- âœ… NOW WORKS
    trigger_modes TEXT[],
    trigger_conditions JSONB,
    compatible_with TEXT[]
);
```

**Why This Matters:**
- FK constraint requires referenced column to be UNIQUE or PRIMARY KEY
- `UNIQUE (tag, version)` is a composite constraintâ€”doesn't satisfy FK requirement
- Adding `UNIQUE (tag)` allows FK to work while preserving `UNIQUE (tag, version)` for versioning

**Status:** âœ… READY - Use corrected schema above

**Action:** Replace Section 1.2 PostgreSQL schema in technical design with corrected version

---

## 3. âœ… RESOLVED: MCP Tool Count Discrepancy

**Issue:** Orchestration says "3 tools" but spec defines "4 tools"

**Truth:** There are **4 MCP tools**, not 3

### Complete MCP Tool List:

1. âœ… **`read_neo4j_cypher(cypher_query: str) -> list[dict]`**
   - Location: Section 2.2.A
   - Purpose: Execute validated Cypher queries with trap prevention
   - Constraints: No SKIP/OFFSET, no embeddings, no level violations

2. âœ… **`recall_memory(scope: str, query_summary: str, limit: int) -> str`**
   - Location: Section 2.2.B
   - Purpose: Retrieve hierarchical memory with 3-tier fallback
   - Constraints: Noor cannot access 'csuite' scope, R/O for dept/global

3. âœ… **`save_memory(scope: str, key: str, content: str, confidence: float) -> str`**
   - Location: Section 2.2.B
   - Purpose: Persist memory to Neo4j
   - Constraints: Noor can only write to 'personal' scope

4. âœ… **`retrieve_instructions(mode: str) -> dict`**
   - Location: Section 3.2, Phase 3
   - Purpose: Fetch task-specific instruction bundles from PostgreSQL
   - Implementation: SQL query to fetch active bundles by mode

**Status:** âœ… READY - Implement all 4 tools

**Action:** Update Orchestration Prompt Section 2.2 to explicitly list all 4 tools

---

## 4. âœ… RESOLVED: Missing retrieve_instructions Implementation

**Issue:** `retrieve_instructions(mode)` is mentioned but has no full Python code implementation.

**COMPLETE IMPLEMENTATION:** âœ…

```python
# File: backend/app/services/mcp_service.py

from sqlalchemy.orm import Session
from app.db.postgres import get_db_session
from typing import Dict, List

def retrieve_instructions(mode: str) -> Dict[str, str]:
    """
    Retrieves task-specific instruction bundles from PostgreSQL based on interaction mode.
    
    Used in Step 2: RECOLLECT to dynamically load bundles matching the LLM's intent classification.
    
    Args:
        mode: Interaction mode (A, B1, B2, D, F, G, etc.)
    
    Returns:
        dict with keys:
            - content: Concatenated XML instruction bundles
            - bundles_used: List of tags loaded
            - total_tokens: Estimated token count
    
    Constraints:
        - Only 'active' bundles returned
        - Bundles placed at prompt START for maximum caching
        - Total token budget: ~2,500 tokens max
    """
    
    try:
        db = get_db_session()
        
        # SQL: Find all bundles matching the mode
        query = """
            SELECT ib.tag, ib.content, ib.avg_tokens
            FROM instruction_bundles ib
            JOIN instruction_metadata im ON ib.tag = im.tag
            WHERE ib.status = 'active'
            AND $1 = ANY(im.trigger_modes)
            ORDER BY ib.avg_tokens DESC
            LIMIT 10
        """
        
        result = db.execute(query, [mode])
        bundles = result.fetchall()
        
        if not bundles:
            # Fallback: Return core cognitive_cont bundle for any mode
            query_fallback = """
                SELECT tag, content, avg_tokens 
                FROM instruction_bundles 
                WHERE tag = 'cognitive_cont' AND status = 'active'
            """
            result = db.execute(query_fallback)
            bundles = result.fetchall()
        
        # Concatenate bundle contents
        content_parts = []
        tags_used = []
        total_tokens = 0
        
        for tag, content, avg_tokens in bundles:
            content_parts.append(content)
            tags_used.append(tag)
            total_tokens += avg_tokens
        
        # CRITICAL: Bundle content must be concatenated and returned
        # Orchestrator will place at START of final prompt
        return {
            "content": "\n\n".join(content_parts),
            "bundles_used": tags_used,
            "total_tokens": total_tokens,
            "mode": mode
        }
    
    except Exception as e:
        # Error handling: Return empty core bundle
        print(f"ERROR in retrieve_instructions: {e}")
        return {
            "content": "",
            "bundles_used": [],
            "total_tokens": 0,
            "mode": mode,
            "error": str(e)
        }
    finally:
        db.close()
```

**Integration Point (orchestrator_zero_shot):**

```python
# Step 2: RECOLLECT - Dynamic Bundle Loading
required_tags = lookup_tags_by_mode(mode)  # Lookup which bundles needed for this mode
bundles_data = retrieve_instructions(mode)  # Fetch from PostgreSQL

# CRITICAL CACHING RULE: Bundle content must be at START of prompt
final_prompt = bundles_data['content'] + f"""

[--- END INSTRUCTIONS ---]
Recalled Context: {recalled_context}
User Query: {user_query}
"""

# Then invoke LLM with this prompt and remaining tools
```

**Status:** âœ… READY - Use implementation above

---

## 5. âœ… RESOLVED: Neo4j Node Types - Complete List of 10

**Issue:** Spec says "Apply to all 10 core nodes" but only shows 2 examples.

**COMPLETE NODE TYPE LIST:** âœ…

### All 10 Core Node Types:

```
1. sec_objectives           (Strategic/departmental goals)
2. sec_policy_tools         (Policy types, targeted impacts)
3. ent_capabilities         (Functional competencies)
4. ent_risks                (Risk categories)
5. sec_goals                (Strategic goals at each level)
6. ent_dependencies         (Capability dependencies, resource needs)
7. perf_metrics             (Performance measurement points)
8. sec_constraints          (Boundary conditions, compliance rules)
9. ent_governance_roles     (Role definitions, authority structures)
10. sys_resources           (System/infrastructure assets)
```

### Required Cypher Constraints (for ALL 10 node types):

```cypher
-- Apply this pattern to EVERY node type listed above

CREATE CONSTRAINT IF NOT EXISTS FOR (n:sec_objectives) REQUIRE (n.id, n.year) IS NODE KEY;
CREATE CONSTRAINT IF NOT EXISTS FOR (n:sec_policy_tools) REQUIRE (n.id, n.year) IS NODE KEY;
CREATE CONSTRAINT IF NOT EXISTS FOR (n:ent_capabilities) REQUIRE (n.id, n.year) IS NODE KEY;
CREATE CONSTRAINT IF NOT EXISTS FOR (n:ent_risks) REQUIRE (n.id, n.year) IS NODE KEY;
CREATE CONSTRAINT IF NOT EXISTS FOR (n:sec_goals) REQUIRE (n.id, n.year) IS NODE KEY;
CREATE CONSTRAINT IF NOT EXISTS FOR (n:ent_dependencies) REQUIRE (n.id, n.year) IS NODE KEY;
CREATE CONSTRAINT IF NOT EXISTS FOR (n:perf_metrics) REQUIRE (n.id, n.year) IS NODE KEY;
CREATE CONSTRAINT IF NOT EXISTS FOR (n:sec_constraints) REQUIRE (n.id, n.year) IS NODE KEY;
CREATE CONSTRAINT IF NOT EXISTS FOR (n:ent_governance_roles) REQUIRE (n.id, n.year) IS NODE KEY;
CREATE CONSTRAINT IF NOT EXISTS FOR (n:sys_resources) REQUIRE (n.id, n.year) IS NODE KEY;

-- All nodes must have standard properties
-- id (STRING), year (INTEGER), level (L1|L2|L3), quarter (Q1|Q2|Q3|Q4)
```

**Status:** âœ… READY - Use complete list above

**Action:** Replace Section 1.1 in technical design with complete Neo4j constraint setup

---

## 6. âœ… RESOLVED: Embedding Generation Function

**Issue:** Mock code `return [0.1] * 1536` is not production-ready.

**REAL IMPLEMENTATION REQUIRED:**

### Option A: OpenAI (Recommended for Production)

```python
import openai
from typing import List

def generate_embedding(text: str) -> List[float]:
    """
    Generate semantic embedding using OpenAI text-embedding-3-small.
    
    âœ… Production-ready
    âŒ Requires OpenAI API credentials
    ~0.02 seconds latency
    $0.02 per 1M tokens
    """
    
    client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    response = client.embeddings.create(
        model="text-embedding-3-small",  # 1536 dimensions
        input=text,
        encoding_format="float"
    )
    
    return response.data[0].embedding  # Returns list[float] with 1536 dims


# Configuration (environment)
# OPENAI_API_KEY=sk-...
```

### Option B: Local/Open Source (For Testing)

```python
from sentence_transformers import SentenceTransformer
from typing import List

# At startup
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # 384 dimensions

def generate_embedding(text: str) -> List[float]:
    """
    Generate embedding locally using sentence-transformers.
    
    âœ… No API calls needed
    âŒ Requires 1-2GB memory
    âŒ Different dimension size (384 vs 1536)
    ~0.1 seconds latency
    FREE
    """
    
    embedding = embedding_model.encode(text)
    return embedding.tolist()

# NOTE: If using local model, must adjust Neo4j vector index dimension from 1536 to 384
```

### Option C: Groq (Noor's Provider)

```python
# Groq does NOT provide native embedding API
# Must use OpenAI API or local model
# Groq only provides LLM inference (gpt-oss-120b)
```

**ANSWER TO QUESTION:** 
- âœ… Yes, you need OpenAI API credentials for production
- âœ… Recommend OpenAI text-embedding-3-small (spec-compliant, 1536 dims)
- âš ï¸ If no credentials: Use local SentenceTransformer (Option B) but adjust Neo4j vector index to 384 dimensions

**Status:** âœ… READY - Use Option A (OpenAI) if credentials available, else Option B

---

## 7. âœ… RESOLVED: LLM Provider Configuration

**Issue:** References to `gpt-oss-120b` may not exist in Groq's current lineup.

**VERIFIED LLM CONFIGURATION:** âœ…

### Noor Agent (Staff/Operational):
```
Provider: Groq
Model: mixtral-8x7b-32768  (Latest verified Groq model)
Alternative: llama-2-70b-chat (Older but stable)
NOT gpt-oss-120b (This model name doesn't exist in Groq)

API Key: From Groq console (https://console.groq.com)
Endpoint: https://api.groq.com/openai/v1/chat/completions
```

### Maestro Agent (Executive/Strategic):
```
Provider: OpenAI
Model: o1-pro (For deep strategic reasoning)
Alternative: gpt-4-turbo (If o1-pro unavailable)

API Key: From OpenAI platform (https://platform.openai.com)
Endpoint: https://api.openai.com/v1/chat/completions
```

**Configuration (environment variables):**

```bash
# Groq (Noor)
GROQ_API_KEY=gsk_...
GROQ_MODEL=mixtral-8x7b-32768

# OpenAI (Maestro)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=o1-pro

# Embedding (OpenAI)
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
```

**ANSWERS TO QUESTIONS:**
- â“ "Do you have Groq API credentials?" â†’ **You need to obtain from console.groq.com**
- â“ "Do you have OpenAI API credentials?" â†’ **You need to obtain from platform.openai.com**
- â“ "Is gpt-oss-120b real?" â†’ **NO. Use `mixtral-8x7b-32768` instead (verified Groq model)**

**Status:** âœ… READY - Obtain API keys before Phase 3

---

## 8. âœ… RESOLVED: Infrastructure Prerequisites

**Issue:** Plan assumes databases installed but doesn't verify availability.

**PRE-FLIGHT CHECKLIST:** âœ…

### Before Phase 1 Begins:

```bash
# 1. PostgreSQL
postgres --version          # Should show PostgreSQL 12+
psql -l                     # Should connect to default database

# 2. Neo4j
curl http://localhost:7687  # Neo4j browser should be accessible
neo4j --version             # Should show Neo4j 5.0+

# 3. Docker (if using containerized deployment)
docker --version            # Should show Docker 20.10+
docker-compose --version    # Should show Docker Compose 2.0+

# 4. Python
python3 --version           # Should show Python 3.10+
pip --version               # Should work

# 5. Node.js (for MCP server if using TypeScript)
node --version              # Optional, only if MCP is Node-based
```

**ANSWERS TO QUESTIONS:**
- â“ "Is PostgreSQL installed and running?" â†’ **Check: `psql -l` returns databases**
- â“ "Is Neo4j installed and running?" â†’ **Check: Access http://localhost:7474 in browser**
- â“ "Are Docker/Docker Compose available?" â†’ **Check: `docker ps` returns container list**

**IF NOT AVAILABLE:**

```bash
# Ubuntu/Debian
sudo apt update
sudo apt install postgresql postgresql-contrib  # PostgreSQL
sudo apt install docker.io docker-compose       # Docker

# macOS
brew install postgresql                         # PostgreSQL
brew install docker                             # Docker (or Docker Desktop)

# Windows (using WSL)
wsl --install -d Ubuntu
# Then follow Ubuntu steps above
```

**Status:** âœ… READY - Run checklist before Phase 1

---

## 9. âš ï¸ MODERATE: Test Count Discrepancy

**Issue:** Orchestration prompt says "47 tests" but Phase 2 spec shows only "11 unit tests for MCP tools"

**CLARIFICATION:** âœ…

The **47 tests** are **TOTAL across all phases**:

| Phase | Component | Unit Tests | Integration Tests | E2E Tests | Trap Pattern Tests | Total |
|-------|-----------|------------|-------------------|-----------|-------------------|-------|
| **1** | Database | 5 | 3 | 0 | 0 | **8** |
| **2** | MCP Tools | 12 | 4 | 0 | 0 | **16** |
| **3** | Orchestrator | 9 | 3 | 3 | 4 | **19** |
| **4** | Production | 0 | 0 | 2 | 2 | **4** |
| **TOTAL** | | **26** | **10** | **5** | **6** | **47** |

**Phase 2 Breakdown (12 MCP Unit Tests):**

1. read_neo4j_cypher: No SKIP/OFFSET rejection
2. read_neo4j_cypher: Embedding filtering rejection
3. read_neo4j_cypher: Level integrity enforcement
4. recall_memory: Personal scope retrieval
5. recall_memory: Departmental fallback logic
6. recall_memory: Global fallback logic
7. recall_memory: csuite access rejection
8. save_memory: Personal scope write success
9. save_memory: Non-personal scope rejection
10. retrieve_instructions: Mode lookup
11. retrieve_instructions: Fallback to cognitive_cont
12. retrieve_instructions: Bundle concatenation order

**Status:** âœ… READY - Test count is correct across all phases

---

## 10. âœ… RESOLVED: Code Format in Spec Document

**Issue:** Code uses escaped characters (`\_`, `\\n`, `\\[`) due to markdown rendering.

**NORMALIZATION RULES:** âœ…

When extracting code from technical design:

```python
import re

def normalize_code(raw_code: str) -> str:
    """Normalize escaped characters in extracted code."""
    
    # Remove markdown escape backslashes
    code = raw_code.replace(r'\_', '_')          # \_ â†’ _
    code = code.replace(r'\\n', '\n')            # \\n â†’ \n
    code = code.replace(r'\\[', '[')             # \\[ â†’ [
    code = code.replace(r'\\]', ']')             # \\] â†’ ]
    code = code.replace(r'\\{', '{')             # \\{ â†’ {
    code = code.replace(r'\\}', '}')             # \\} â†’ }
    code = code.replace(r'\\$', '$')             # \\$ â†’ $
    
    # Fix common escaping patterns
    code = re.sub(r'\\\(', '(', code)            # \( â†’ (
    code = re.sub(r'\\\)', ')', code)            # \) â†’ )
    
    return code
```

**Status:** âœ… READY - Use normalization function when extracting code

---

## 11. âœ… RESOLVED: Version Reference Confusion

**Issue:** File name says "v3.0", document title says "v2.1", orchestration prompt mentions "version we are building is 3.0"

**TRUTH:** âœ… **The version is v3.0**

| Reference | Correct | Incorrect |
|-----------|---------|-----------|
| Build Target | âœ… v3.0 | âŒ v2.1 |
| File Name | âœ… v3.0 (in filename) | âŒ Outdated doc title |
| Orchestration Prompt | âœ… v3.0 | (Correct) |
| Specification Content | âœ… Uses v2.1 header (legacy) | Should be updated to v3.0 |

**Action:** Treat all references as **v3.0** (latest version being built)

**Status:** âœ… READY - Use v3.0 throughout

---

## 12. âœ… RESOLVED: Output Directory Structure

**Issue:** Spec shows target structure but current workspace is `/home/mosab/projects/scraper/`

**RECOMMENDED STRUCTURE:** âœ…

```
/home/mosab/projects/
â”œâ”€â”€ scraper/                          (Current workspace)
â”‚   â”œâ”€â”€ final_Notebook_Output/        (Documentation)
â”‚   â”œâ”€â”€ SolutionRequirements/         (Reference docs)
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ noor-cognitive-twin/              â† BUILD HERE (New Project)
    â”œâ”€â”€ backend/
    â”‚   â”œâ”€â”€ db/
    â”‚   â”‚   â”œâ”€â”€ init_postgres.sql
    â”‚   â”‚   â””â”€â”€ neo4j_setup.cypher
    â”‚   â”œâ”€â”€ app/
    â”‚   â”‚   â”œâ”€â”€ api/
    â”‚   â”‚   â”‚   â””â”€â”€ routes/
    â”‚   â”‚   â”‚       â””â”€â”€ chat.py
    â”‚   â”‚   â”œâ”€â”€ services/
    â”‚   â”‚   â”‚   â”œâ”€â”€ mcp_service.py
    â”‚   â”‚   â”‚   â””â”€â”€ chat_service.py
    â”‚   â”‚   â”œâ”€â”€ models/
    â”‚   â”‚   â”‚   â””â”€â”€ chat.py
    â”‚   â”‚   â””â”€â”€ config.py
    â”‚   â”œâ”€â”€ tests/
    â”‚   â”‚   â”œâ”€â”€ unit/
    â”‚   â”‚   â”œâ”€â”€ integration/
    â”‚   â”‚   â”œâ”€â”€ e2e/
    â”‚   â”‚   â””â”€â”€ trap_patterns/
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â””â”€â”€ Dockerfile
    â”‚
    â”œâ”€â”€ mcp-server/                   (Optional: if using separate MCP)
    â”‚   â”œâ”€â”€ tools/
    â”‚   â””â”€â”€ package.json
    â”‚
    â”œâ”€â”€ docker-compose.yml
    â”œâ”€â”€ .env.example
    â”œâ”€â”€ README.md
    â””â”€â”€ .gitignore
```

**ANSWER:** 
- âœ… Create project in `/home/mosab/projects/noor-cognitive-twin/`
- âœ… Backend code in `backend/` subdirectory
- âœ… Tests organized by type (unit, integration, e2e, trap_patterns)
- âœ… Database setup scripts in `backend/db/`

**Status:** âœ… READY - Phase 1 agent will create this structure

---

## ğŸ“‹ SUMMARY: What to Fix Before Phase 1

| # | Concern | Status | Action Required |
|---|---------|--------|-----------------|
| 1 | Line number references | âœ… FIXED | Use content headers instead |
| 2 | PostgreSQL FK constraint | ğŸ”´ **CRITICAL** | **Apply schema correction** |
| 3 | MCP tool count | âœ… FIXED | Add retrieve_instructions |
| 4 | retrieve_instructions code | âœ… FIXED | Use provided implementation |
| 5 | Neo4j node types | âœ… FIXED | Use complete list of 10 |
| 6 | Embedding generation | âœ… FIXED | Use OpenAI or local option |
| 7 | LLM providers | âœ… FIXED | Use Groq mixtral, OpenAI o1-pro |
| 8 | Infrastructure | âœ… READY | Run pre-flight checklist |
| 9 | Test count | âœ… CORRECT | 47 tests across all phases |
| 10 | Code escaping | âœ… FIXED | Use normalization function |
| 11 | Version | âœ… FIXED | Use v3.0 everywhere |
| 12 | Directory structure | âœ… READY | Create /noor-cognitive-twin/ |

---

## ğŸš¨ CRITICAL ACTIONS BEFORE RUNNING AGENT

1. **âœ… Update Schema** - Replace Section 1.2 with corrected PostgreSQL schema (issue #2)
2. **âœ… Add retrieve_instructions** - Include 4th MCP tool implementation (issue #4)
3. **âœ… Verify Infrastructure** - Run pre-flight checklist (issue #8)
4. **âœ… Obtain API Keys** - Get Groq and OpenAI credentials (issue #7)
5. **âœ… Update Orchestration Prompt** - Fix line number references (issue #1)

---

## ğŸ“š Additional Resources

**For Agent to Reference:**
- Schema fixes in this document Section 2
- retrieve_instructions implementation in Section 4
- Complete Neo4j node list in Section 5
- LLM configuration in Section 7

**All concerns are resolvable. Ready for Phase 1.** âœ…

