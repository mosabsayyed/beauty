This detailed report addresses your four questions regarding OpenAI's Structured Outputs and multi-turn conversation management, drawing specifically on the available documentation.

---

## **1\. OpenAI Structured Outputs with `response_format`**

Structured Outputs represent the evolution of the older JSON Mode. While both guarantee valid JSON output, **only Structured Outputs ensure strict adherence to the defined schema**. The key to forcing the AI to return the exact structure required, without solely relying on text instructions, is the use of the `json_schema` type with the `strict: true` parameter.

### **Key Components for Strict Adherence**

1. **Model Compatibility:** Structured Outputs are only supported by certain models, including `gpt-4o-mini-2024-07-18` and later, and `gpt-4o-2024-08-06` and later.  
2. **`response_format` Parameter:** This must be set in the API call to specify JSON schema adherence.  
3. **`strict: true`:** Setting `"strict": true` guarantees that the JSON output will conform 100% of the time to the schema provided. If this flag is omitted, the API might return a status 200 OK, but the response body might not fully conform, leading to empty or null parsing results.  
4. **Schema Constraints:** The schema must include the constraints `"required"` and `"additionalProperties": false` in all object definitions to prevent the model from generating undefined fields.

### **Python Code Example (using raw JSON schema)**

The following example shows how to enforce the structured output using Python (this structure is based on cURL and theoretical Python implementations shown in the sources).

import json  
import os  
from openai import OpenAI, BadRequestError \# Import specific error classes

\# \--- 1\. Define the Schema (Raw JSON Schema) \---  
\# Note the required use of "strict": true and "additionalProperties": false  
SCHEMA\_DEFINITION \= {  
    "name": "CalendarEventResponse",  
    "strict": True,  
    "schema": {  
        "type": "object",  
        "properties": {  
            "name": {"type": "string"},  
            "date": {"type": "string"},  
            "participants": {  
                "type": "array",  
                "items": {"type": "string"}  
            }  
        },  
        "required": \["name", "date", "participants"\],  
        "additionalProperties": False  
    }  
}

client \= OpenAI(api\_key=os.environ.get("OPENAI\_API\_KEY"))

def get\_structured\_event():  
    try:  
        \# \--- 2\. Call API with response\_format and messages \---  
        response \= client.chat.completions.create(  
            model="gpt-4o-2024-08-06", \# Must use a supported model  
            messages=\[  
                \# It's still best practice to instruct the model to output JSON  
                {"role": "system", "content": "You are a helpful assistant that extracts event information and returns only JSON."},  
                {"role": "user", "content": "Alice and Bob are going to a science fair on Friday."}  
            \],  
            response\_format={  
                "type": "json\_schema",  
                "json\_schema": SCHEMA\_DEFINITION  
            }  
        )

        \# \--- 3\. Parse the Guaranteed Response \---  
        \# The content field contains the validated JSON string  
        json\_string \= response.choices.message.content  
        return json.loads(json\_string)

    \# \--- 4\. Error Handling for Refusals / Invalid Requests \---  
    except BadRequestError as e:  
        \# This error is raised if the schema is invalid (e.g., missing required fields,  
        \# using unsupported keywords like 'default'), or if the model refuses due to safety.  
        print(f"API Request Failed (400 Bad Request): {e.message}")  
        return None  
    except Exception as e:  
        print(f"An unexpected error occurred: {e}")  
        return None

\# print(get\_structured\_event())

## **2\. Multi-turn Conversation Management**

The OpenAI API is **stateless**, meaning it does not retain history between calls. The developer must manage the conversation context and send the full history with every subsequent request.

### **Exact Format for Sending Message History**

The conversation history is passed in the `messages` array, which is a list of objects, each containing a `role` and `content`:

* **`system`**: Sets the agent's persona, behavior, or core instructions, and its instructions are prioritized over the user messages.  
* **`user`**: Represents the human user's queries or commands.  
* **`assistant`**: Represents the previous responses generated by the LLM.

### **How to Append Assistant Responses**

To maintain coherence, you must append both the user's new input and the assistant's previous output to the `messages` array before calling the API again:

1. User sends message N.  
2. The full history (messages 1 to N-1 \+ User message N) is sent to the API.  
3. The response is received.  
4. The assistant's response is appended to the history with `role: "assistant"`.  
5. Repeat.

### **Best Practices for Managing Context Window Limits**

LLMs, such as GPT-4, have finite context window limits (e.g., 8,192 tokens). Sending the entire conversation history repeatedly can lead to exponentially increasing costs and slow down responses. Strategies to manage this include:

| Strategy | Description | Source |
| ----- | ----- | ----- |
| **Rolling Context/Window** | Only return the most recent $X$ messages and discard older ones from the start of the array to stay within the token limit. |  |
| **Summarised Message History** | Use the LLM to periodically summarize older conversation segments into a single, succinct summary, and replace the detailed messages with this summary in the history array. |  |
| **External Memory/Vector DB** | Store conversation embeddings in a vector database and retrieve only contextually relevant past exchanges dynamically, minimizing the data sent in the prompt. |  |

### **Code Example Showing 3+ Turns (Node.js/TypeScript structure replicated, as Python client examples were limited)**

The pattern involves continuously appending to a `messageHistory` array:

\# Initialization  
messageHistory \= \[  
    {"role": "system", "content": "You are a friendly bot."},  
\]

\# \--- Turn 1 \---  
user\_message\_1 \= "I bought bread and milk. How many tokens did I spend?"  
messageHistory.append({"role": "user", "content": user\_message\_1})

\# API Call 1 (sends system \+ user\_message\_1)  
\# response\_1 \= client.chat.completions.create(messages=messageHistory, ...)  
assistant\_content\_1 \= "You spent 15 tokens on groceries."  
messageHistory.append({"role": "assistant", "content": assistant\_content\_1})

\# \--- Turn 2 \---  
user\_message\_2 \= "Did I buy cheese?" \# Context relies on previous messages  
messageHistory.append({"role": "user", "content": user\_message\_2})

\# API Call 2 (sends system \+ user\_message\_1 \+ assistant\_content\_1 \+ user\_message\_2)  
\# response\_2 \= client.chat.completions.create(messages=messageHistory, ...)  
assistant\_content\_2 \= "No, you only mentioned bread and milk."  
messageHistory.append({"role": "assistant", "content": assistant\_content\_2})

\# \--- Turn 3 \---  
\# ... and so on, with the full history being sent each time ...

## **3\. Combining Structured Outputs with Conversation History**

These two features are combined by applying the `response_format` to the *API call* while simultaneously managing the *input context* via the `messages` array for that call.

### **Complete Working Example (Conceptual Synthesis)**

This demonstrates using the strict JSON schema from Q1 while feeding the history from Q2.

import json  
from openai import OpenAI  
from pydantic import BaseModel  
from typing import List

\# \--- Define Schema using Pydantic (Preferred Method, see Q4) \---  
class ActionItem(BaseModel):  
    task: str  
    due\_date: str  
    priority: int

class ActionPlan(BaseModel):  
    action\_items: List\[ActionItem\]  
    overall\_summary: str

\# Schema definition for response\_format (if using client.chat.completions.create)  
JSON\_SCHEMA \= {  
    "type": "json\_schema",  
    "json\_schema": {  
        "strict": True,  
        "schema": ActionPlan.model\_json\_schema(),  
        "name": "ActionPlan"  
    }  
}

client \= OpenAI(api\_key="...")

\# Initialize conversation history with prior context  
conversation\_history \= \[  
    {"role": "system", "content": "You are a strict Project Manager AI. Always output action plans."},  
    {"role": "user", "content": "I finished the Q1 budget report."},  
    {"role": "assistant", "content": '{"tasks\_completed": 1, "status": "Good job."}'},  
\]

\# \--- Turn 4: User requests structured output \---  
current\_query \= "Now, based on that, create an action plan for the next two weeks related to product launch."  
conversation\_history.append({"role": "user", "content": current\_query})

\# API Call: Sends full history and enforces strict JSON output  
response \= client.chat.completions.create(  
    model="gpt-4o-2024-08-06",  
    messages=conversation\_history, \# Full multi-turn history is sent  
    response\_format=JSON\_SCHEMA  
)

\# Parse and continue conversation  
new\_content \= response.choices.message.content  
print("Strict JSON Output:", new\_content)

\# Append the new structured response to maintain history  
conversation\_history.append({"role": "assistant", "content": new\_content})  
\# print(conversation\_history) \# History is ready for the next turn

### **Structure and Role Compatibility**

* **Structuring the `messages` array:** The array is structured sequentially with alternating `user` and `assistant` roles, starting with the `system` prompt. When the `response_format` is active, the content of the latest (unresolved) message in the array is typically the user's request for the structured information.  
* **Role Compatibility:** All roles (`system`, `user`, `assistant`) function correctly with structured outputs. The `system` and `user` messages provide the necessary context for the model to generate its response, and the `response_format` applies the structure constraints to the final output generated by the LLM (which will have the `assistant` role).

## **4\. Pydantic Models for OpenAI Structured Outputs**

Pydantic is highly valuable because it defines a schema using Python type hints, simplifying data validation, and can automatically generate the necessary JSON Schema for the API.

### **Python Code Pattern using `client.beta.chat.completions.parse()`**

The modern OpenAI Python SDK introduces `client.beta.chat.completions.parse()`, which handles the Pydantic-to-JSON Schema conversion and automatic parsing of the output. This abstracts away the need for manual `json.loads()` and `model_validate_json()` calls.

import json  
from pydantic import BaseModel, Field  
from typing import List, Optional, Union, Literal  
from openai import OpenAI, BadRequestError  
from datetime import datetime

\# \--- 1\. Define Nested Pydantic Models \---  
\# Nested models are defined as fields within other BaseModel classes.  
class Publisher(BaseModel):  
    name: str \= Field(description="The name of the publisher")  
    \# Using Union\[str, None\] or Optional\[str\] handles optional fields  
    url: Optional\[str\] \= Field(None, description="The URL of the publisher's website")

class Article(BaseModel):  
    title: str \= Field(description="The title of the news article")  
    \# Handling Optional Fields (Note: Defaults like "= None" are typical Pydantic practice)  
    \# However, the OpenAI API may reject schemas with explicit 'default' fields.  
    publisher: Optional\[Publisher\] \= None \# Optional Nested Structure

class NewsArticles(BaseModel):  
    query: str \= Field(description="The query used to search for news articles")  
    articles: List\[Article\] \= Field(description="The list of news articles returned by the query") \# Handles Lists  
    \# Use Literal or Enum to constrain values strictly  
    language: Literal\["English", "Spanish"\] \= "English"

client \= OpenAI(api\_key="...")

def extract\_articles(query: str, history: List\[dict\]):  
    try:  
        \# \--- 2\. Use client.beta.chat.completions.parse() \---  
        \# Passing the Pydantic model directly to response\_format handles schema generation.  
        completion \= client.beta.chat.completions.parse(  
            model="gpt-4o-2024-08-06",  
            messages=history \+ \[{"role": "user", "content": query}\],  
            response\_format=NewsArticles, \# Pass Pydantic Model here  
        )

        \# \--- 3\. Parse the Guaranteed Response \---  
        \# The SDK automatically validates and returns the Pydantic object instance in the 'parsed' attribute.  
        result\_object \= completion.choices.message.parsed

        \# Ensure it is the correct type  
        if isinstance(result\_object, NewsArticles):  
            \# No manual json.loads() or model\_validate\_json() needed  
            return result\_object.articles  
        return None

    \# \--- 4\. Error Handling for Parsing and API Issues \---  
    except BadRequestError as e:  
        \# Catches API errors, often due to schema issues or invalid parameter usage.  
        print(f"API Error (Schema/Model Issue): {e.message}")  
        \# Workarounds for schema incompatibility (like handling 'default' fields) may be necessary.  
        return \[\]  
    except Exception as e:  
        \# Catches unexpected errors, including Pydantic ValidationErrors if JSON is malformed.  
        print(f"Extraction failed: {e}")  
        return \[\]

### **Note on Optional Fields and Defaults**

While Pydantic natively supports optional fields using `Optional` or `Union`, and fields with default values (e.g., `field: str = "DEFAULT"`), the **OpenAI API validation currently rejects JSON schemas that contain the `'default'` field**. Developers frequently rely on wrappers or manual patching to remove the `'default'` property from the generated JSON schema to avoid receiving a `BadRequestError`. Fields that are optional should be defined but should not have a `default` value set in the model if you rely on the API's automatic schema generation to maintain strict compatibility.

